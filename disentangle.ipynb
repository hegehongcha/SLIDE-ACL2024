{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42c2fd-b748-4d77-843a-57a7d476611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import IterableDataset\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "with open (\"train.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "train_examples = []\n",
    "for line in lines:\n",
    "    # print(line)\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "            neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "            train_examples.append(InputExample(texts=[context, pos, neg]))\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "model_name = \"sentence-t5-base\"\n",
    "\n",
    "train_batch_size = 16           #Increasing the train batch size improves the model performance, but requires more GPU memory\n",
    "max_seq_length = 512            #Max length for passages. Increasing it, requires more GPU memory\n",
    "# ce_score_margin = args.ce_score_margin             #Margin for the CrossEncoder score between negative and positive passages\n",
    "# num_negs_per_system = args.num_negs_per_system         # We used different systems to mine hard negatives. Number of hard negatives to add from each system\n",
    "num_epochs = 10                 # Number of epochs we want to train\n",
    "\n",
    "use_pre_trained_model = True\n",
    "# Load our embedding model\n",
    "if use_pre_trained_model:\n",
    "    logging.info(\"use pretrained SBERT model\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.max_seq_length = max_seq_length\n",
    "else:\n",
    "    logging.info(\"Create new SBERT model\")\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), args.pooling)\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "model_save_path = 'output/train_bi-encoder-mnrl-disentangle-{}-{}'.format(model_name.replace(\"/\", \"-\"),  datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "\n",
    "### Now we read the MS Marco dataset\n",
    "# data_folder = 'msmarco-data'\n",
    "\n",
    "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
    "# train_dataset = MSMARCODataset(train_queries, corpus=corpus)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=1000,\n",
    "          use_amp=True,\n",
    "          checkpoint_path=model_save_path,\n",
    "          checkpoint_save_steps=len(train_dataloader),\n",
    "          optimizer_params = {'lr': 2e-5},\n",
    "          )\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303bc25-021e-4c38-9b82-ae2a3189dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.先对文本进行constrastive learning，并对sentence transformer模型的输出加一层线性层\n",
    "# 2.对线性层的输出再进行一次contrastive learning，保证线性层的输出与sentence transformer模型的输出差别不大，此时冻结sentence transformer模型训练，\n",
    "#   包括以后的步骤也会冻结sentence transformer模型\n",
    "# 3，对线性层的输出进行切分：robust与non robust，并做contrastive learning\n",
    "#    3.1 response内部contrastive learning：robust与non robust远离\n",
    "#    3.2 不同response之间contrastive learning：robust与robust相近\n",
    "#  对于这一步，设立一个分类器\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590b875-eb25-4ba5-a3fa-7087cb4679e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import IterableDataset\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from enum import Enum\n",
    "# import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b5a19-c0d2-448b-a817-cbe98cfbf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        # print(hidden_states.shape)\n",
    "        n = hidden_states.shape[0]\n",
    "        a = torch.ones(n)\n",
    "        labels = labels * a\n",
    "        labels = labels.long()\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "class SiameseDistanceMetric(Enum):\n",
    "    \"\"\"\n",
    "    The metric for the contrastive loss\n",
    "    \"\"\"\n",
    "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
    "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n",
    "    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n",
    "\n",
    "class Second_MultipleNegativesRankingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        This loss expects as input a batch consisting of sentence pairs (a_1, p_1), (a_2, p_2)..., (a_n, p_n)\n",
    "        where we assume that (a_i, p_i) are a positive pair and (a_i, p_j) for i!=j a negative pair.\n",
    "\n",
    "        For each a_i, it uses all other p_j as negative samples, i.e., for a_i, we have 1 positive example (p_i) and\n",
    "        n-1 negative examples (p_j). It then minimizes the negative log-likehood for softmax normalized scores.\n",
    "\n",
    "        This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc))\n",
    "        as it will sample in each batch n-1 negative docs randomly.\n",
    "\n",
    "        The performance usually increases with increasing batch sizes.\n",
    "\n",
    "        For more information, see: https://arxiv.org/pdf/1705.00652.pdf\n",
    "        (Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4)\n",
    "\n",
    "        You can also provide one or multiple hard negatives per anchor-positive pair by structering the data like this:\n",
    "        (a_1, p_1, n_1), (a_2, p_2, n_2)\n",
    "\n",
    "        Here, n_1 is a hard negative for (a_1, p_1). The loss will use for the pair (a_i, p_i) all p_j (j!=i) and all n_j as negatives.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            model = SentenceTransformer('distilbert-base-uncased')\n",
    "            train_examples = [InputExample(texts=['Anchor 1', 'Positive 1']),\n",
    "                InputExample(texts=['Anchor 2', 'Positive 2'])]\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "            train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: float = 20.0, similarity_fct = util.cos_sim):\n",
    "        \"\"\"\n",
    "        :param model: SentenceTransformer model\n",
    "        :param scale: Output of similarity function is multiplied by scale value\n",
    "        :param similarity_fct: similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot product (and then set scale to 1)\n",
    "        \"\"\"\n",
    "        super(Second_MultipleNegativesRankingLoss, self).__init__()\n",
    "        # self.model = model\n",
    "        self.scale = scale\n",
    "        self.similarity_fct = similarity_fct\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # self.distance_metric = SiameseDistanceMetric.COSINE_DISTANCE\n",
    "        # self.classify_model = nn.Linear(2 * model.get_sentence_embedding_dimension(), 3)\n",
    "\n",
    "\n",
    "    def forward(self, reps):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        embeddings_a = reps[0]\n",
    "        embeddings_b = torch.cat(reps[1:])\n",
    "        # print(embeddings_a.shape, embeddings_b.shape)\n",
    "        scores = self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n",
    "        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)  # Example a[i] should match with b[i]\n",
    "        return self.cross_entropy_loss(scores, labels)\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {'scale': self.scale, 'similarity_fct': self.similarity_fct.__name__}\n",
    "\n",
    "class test_ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the\n",
    "    two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.\n",
    "\n",
    "    Further information: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "\n",
    "    :param model: SentenceTransformer model\n",
    "    :param distance_metric: Function that returns a distance between two embeddings. The class SiameseDistanceMetric contains pre-defined metrices that can be used\n",
    "    :param margin: Negative samples (label == 0) should have a distance of at least the margin value.\n",
    "    :param size_average: Average by the size of the mini-batch.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        train_examples = [\n",
    "            InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
    "            InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
    "\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "        train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "        model.fit([(train_dataloader, train_loss)], show_progress_bar=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, distance_metric=SiameseDistanceMetric.COSINE_DISTANCE, margin: float = 0.5, size_average:bool = True):\n",
    "        super(test_ContrastiveLoss, self).__init__()\n",
    "        self.distance_metric = distance_metric\n",
    "        self.margin = margin\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        distance_metric_name = self.distance_metric.__name__\n",
    "        for name, value in vars(SiameseDistanceMetric).items():\n",
    "            if value == self.distance_metric:\n",
    "                distance_metric_name = \"SiameseDistanceMetric.{}\".format(name)\n",
    "                break\n",
    "\n",
    "        return {'distance_metric': distance_metric_name, 'margin': self.margin, 'size_average': self.size_average}\n",
    "\n",
    "    def forward(self, reps, labels):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        # assert len(reps) == 2\n",
    "        labels = torch.tensor(labels)\n",
    "        rep_anchor, rep_other = reps\n",
    "        # print(rep_anchor.shape,rep_other.shape)\n",
    "\n",
    "        distances = self.distance_metric(rep_anchor, rep_other)\n",
    "        # print(distances)\n",
    "        losses = 0.5 * (labels.float() * distances.pow(2) + (1 - labels).float() * F.relu(self.margin - distances).pow(2))\n",
    "        # print(losses)\n",
    "        return losses.mean() if self.size_average else losses.sum()\n",
    "\n",
    "model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-bert-base-uncased-2023-08-29_15-22-44/86808\").cuda()\n",
    "class Disentangle_Model(nn.Module):\n",
    "    def __init__(self, max_seq_length=1024, batch_size=32, num_labels=3):\n",
    "        super(Disentangle_Model, self).__init__()\n",
    "        # self.model_name_or_path = model_name_or_path\n",
    "        # self.device = device\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_labels = num_labels\n",
    "        # self.model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-bert-base-uncased-2023-08-29_15-22-44/86808\")\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        self.second_mutiple_negatives_ranking_loss = Second_MultipleNegativesRankingLoss()\n",
    "        self.contrastive_loss = test_ContrastiveLoss()\n",
    "        self.classifier = Classifier(int(1.5 * 768), self.num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        # self.optimizer = AdamW(self.parameters(), lr=2e-5, eps=1e-8)\n",
    "        # self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=10000)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # self.train()\n",
    "        # print(len(batch[0]))\n",
    "        with torch.no_grad():\n",
    "            context_embedding = torch.tensor(model_encoder.encode(batch[0])).cuda()\n",
    "            pos_response_embedding = torch.tensor(model_encoder.encode(batch[1])).cuda()\n",
    "            neg_response_embedding = torch.tensor(model_encoder.encode(batch[2])).cuda()\n",
    "        # print(neg_response_embedding.device)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        context_embedding = self.linear(context_embedding)\n",
    "        pos_response_embedding = self.linear(pos_response_embedding)\n",
    "        neg_response_embedding = self.linear(neg_response_embedding)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        reps = [context_embedding, pos_response_embedding, neg_response_embedding]\n",
    "        contrastive_loss_1 = self.second_mutiple_negatives_ranking_loss(reps)\n",
    "        # 3.对线性层的输出进行切分：robust与non robust，并做contrastive learning\n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "        pos = [pos_non_robust_embedding, pos_robust_embedding]\n",
    "        # print(pos_non_robust_embedding.shape, pos_robust_embedding.shape)\n",
    "        pos_inside_contrastive_loss = self.contrastive_loss(pos, 0.0)\n",
    "        neg = [neg_non_robust_embedding, neg_robust_embedding]\n",
    "        neg_inside_contrastive_loss = self.contrastive_loss(neg, 0.0)\n",
    "        outside_robust_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_robust_embedding], 0.0)\n",
    "        # outside_diff_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_non_robust_embedding], 0.0)\n",
    "        outside_non_robust_contrastive_loss = self.contrastive_loss([pos_non_robust_embedding, neg_non_robust_embedding], 1.0)\n",
    "        # 4.对于这一步，设立一个分类器\n",
    "        hidden_state = torch.cat([context_embedding, pos_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 1)\n",
    "        classification_loss_1 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, pos_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_2 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 0)\n",
    "        classification_loss_3 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_4 = outputs[1]\n",
    "\n",
    "        loss = contrastive_loss_1 + pos_inside_contrastive_loss + neg_inside_contrastive_loss + outside_robust_contrastive_loss + outside_non_robust_contrastive_loss + classification_loss_1 + classification_loss_2 + classification_loss_3 + classification_loss_4\n",
    "        return loss\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abebc60-a115-47c3-8c20-0a47f8628911",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros([30-20])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4690-579f-483a-8272-4637ed40d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequence,补0到同一长度，并对补0后的robust rep与context做contrastive learning\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        # print(hidden_states.shape)\n",
    "        n = hidden_states.shape[0]\n",
    "        a = torch.ones(n)\n",
    "        labels = labels * a\n",
    "        labels = labels.long()\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "class SiameseDistanceMetric(Enum):\n",
    "    \"\"\"\n",
    "    The metric for the contrastive loss\n",
    "    \"\"\"\n",
    "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
    "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n",
    "    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n",
    "\n",
    "class Second_MultipleNegativesRankingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        This loss expects as input a batch consisting of sentence pairs (a_1, p_1), (a_2, p_2)..., (a_n, p_n)\n",
    "        where we assume that (a_i, p_i) are a positive pair and (a_i, p_j) for i!=j a negative pair.\n",
    "\n",
    "        For each a_i, it uses all other p_j as negative samples, i.e., for a_i, we have 1 positive example (p_i) and\n",
    "        n-1 negative examples (p_j). It then minimizes the negative log-likehood for softmax normalized scores.\n",
    "\n",
    "        This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc))\n",
    "        as it will sample in each batch n-1 negative docs randomly.\n",
    "\n",
    "        The performance usually increases with increasing batch sizes.\n",
    "\n",
    "        For more information, see: https://arxiv.org/pdf/1705.00652.pdf\n",
    "        (Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4)\n",
    "\n",
    "        You can also provide one or multiple hard negatives per anchor-positive pair by structering the data like this:\n",
    "        (a_1, p_1, n_1), (a_2, p_2, n_2)\n",
    "\n",
    "        Here, n_1 is a hard negative for (a_1, p_1). The loss will use for the pair (a_i, p_i) all p_j (j!=i) and all n_j as negatives.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            model = SentenceTransformer('distilbert-base-uncased')\n",
    "            train_examples = [InputExample(texts=['Anchor 1', 'Positive 1']),\n",
    "                InputExample(texts=['Anchor 2', 'Positive 2'])]\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "            train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: float = 20.0, similarity_fct = util.cos_sim):\n",
    "        \"\"\"\n",
    "        :param model: SentenceTransformer model\n",
    "        :param scale: Output of similarity function is multiplied by scale value\n",
    "        :param similarity_fct: similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot product (and then set scale to 1)\n",
    "        \"\"\"\n",
    "        super(Second_MultipleNegativesRankingLoss, self).__init__()\n",
    "        # self.model = model\n",
    "        self.scale = scale\n",
    "        self.similarity_fct = similarity_fct\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # self.distance_metric = SiameseDistanceMetric.COSINE_DISTANCE\n",
    "        # self.classify_model = nn.Linear(2 * model.get_sentence_embedding_dimension(), 3)\n",
    "\n",
    "\n",
    "    def forward(self, reps):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        embeddings_a = reps[0]\n",
    "        embeddings_b = torch.cat(reps[1:])\n",
    "        # print(embeddings_a.shape, embeddings_b.shape)\n",
    "        scores = self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n",
    "        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)  # Example a[i] should match with b[i]\n",
    "        return self.cross_entropy_loss(scores, labels)\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {'scale': self.scale, 'similarity_fct': self.similarity_fct.__name__}\n",
    "\n",
    "class test_ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the\n",
    "    two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.\n",
    "\n",
    "    Further information: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "\n",
    "    :param model: SentenceTransformer model\n",
    "    :param distance_metric: Function that returns a distance between two embeddings. The class SiameseDistanceMetric contains pre-defined metrices that can be used\n",
    "    :param margin: Negative samples (label == 0) should have a distance of at least the margin value.\n",
    "    :param size_average: Average by the size of the mini-batch.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        train_examples = [\n",
    "            InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
    "            InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
    "\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "        train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "        model.fit([(train_dataloader, train_loss)], show_progress_bar=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, distance_metric=SiameseDistanceMetric.COSINE_DISTANCE, margin: float = 0.5, size_average:bool = True):\n",
    "        super(test_ContrastiveLoss, self).__init__()\n",
    "        self.distance_metric = distance_metric\n",
    "        self.margin = margin\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        distance_metric_name = self.distance_metric.__name__\n",
    "        for name, value in vars(SiameseDistanceMetric).items():\n",
    "            if value == self.distance_metric:\n",
    "                distance_metric_name = \"SiameseDistanceMetric.{}\".format(name)\n",
    "                break\n",
    "\n",
    "        return {'distance_metric': distance_metric_name, 'margin': self.margin, 'size_average': self.size_average}\n",
    "\n",
    "    def forward(self, reps, labels):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        # assert len(reps) == 2\n",
    "        labels = torch.tensor(labels)\n",
    "        rep_anchor, rep_other = reps\n",
    "        # print(rep_anchor.shape,rep_other.shape)\n",
    "\n",
    "        distances = self.distance_metric(rep_anchor, rep_other)\n",
    "        # print(distances)\n",
    "        losses = 0.5 * (labels.float() * distances.pow(2) + (1 - labels).float() * F.relu(self.margin - distances).pow(2))\n",
    "        # print(losses)\n",
    "        return losses.mean() if self.size_average else losses.sum()\n",
    "\n",
    "model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-distilbert-base-uncased-2023-08-23_21-32-34/72340\").cuda()\n",
    "class Disentangle_Model(nn.Module):\n",
    "    def __init__(self, max_seq_length=1024, batch_size=32, num_labels=3):\n",
    "        super(Disentangle_Model, self).__init__()\n",
    "        # self.model_name_or_path = model_name_or_path\n",
    "        # self.device = device\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_labels = num_labels\n",
    "        # self.model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-bert-base-uncased-2023-08-29_15-22-44/86808\")\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        self.second_mutiple_negatives_ranking_loss = Second_MultipleNegativesRankingLoss()\n",
    "        self.contrastive_loss = test_ContrastiveLoss()\n",
    "        self.classifier = Classifier(1536, self.num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        # self.optimizer = AdamW(self.parameters(), lr=2e-5, eps=1e-8)\n",
    "        # self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=10000)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # self.train()\n",
    "        # print(len(batch[0]))\n",
    "        with torch.no_grad():\n",
    "            context_embedding = torch.tensor(model_encoder.encode(batch[0])).cuda()\n",
    "            pos_response_embedding = torch.tensor(model_encoder.encode(batch[1])).cuda()\n",
    "            neg_response_embedding = torch.tensor(model_encoder.encode(batch[2])).cuda()\n",
    "        # print(neg_response_embedding.device)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        context_embedding = self.linear(context_embedding)\n",
    "        pos_response_embedding = self.linear(pos_response_embedding)\n",
    "        neg_response_embedding = self.linear(neg_response_embedding)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        reps = [context_embedding, pos_response_embedding, neg_response_embedding]\n",
    "        contrastive_loss_1 = self.second_mutiple_negatives_ranking_loss(reps)\n",
    "        # 3.对线性层的输出进行切分：robust与non robust，并做contrastive learning\n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "\n",
    "        pos_pad = torch.zeros([pos_robust_embedding.size(0), 384]).cuda()\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, pos_pad], dim=1)\n",
    "        pos_non_robust_embedding = torch.cat([pos_non_robust_embedding, pos_pad], dim=1)\n",
    "\n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "        neg_robust_embedding = torch.cat([neg_robust_embedding, pos_pad], dim=1)\n",
    "        neg_non_robust_embedding = torch.cat([neg_non_robust_embedding, pos_pad], dim=1)\n",
    "        pos = [pos_non_robust_embedding, pos_robust_embedding]\n",
    "        # print(pos_non_robust_embedding.shape, pos_robust_embedding.shape)\n",
    "        pos_inside_contrastive_loss = self.contrastive_loss(pos, 0.0)\n",
    "        neg = [neg_non_robust_embedding, neg_robust_embedding]\n",
    "        neg_inside_contrastive_loss = self.contrastive_loss(neg, 0.0)\n",
    "        outside_robust_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_robust_embedding], 0.0)\n",
    "        # outside_diff_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_non_robust_embedding], 0.0)\n",
    "        outside_non_robust_contrastive_loss = self.contrastive_loss([pos_non_robust_embedding, neg_non_robust_embedding], 1.0)\n",
    "        \n",
    "        reps = [context_embedding, pos_robust_embedding, neg_robust_embedding]\n",
    "        contrastive_loss_2 = self.second_mutiple_negatives_ranking_loss(reps)\n",
    "        # 4.对于这一步，设立一个分类器\n",
    "        hidden_state = torch.cat([context_embedding, pos_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 1)\n",
    "        classification_loss_1 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, pos_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_2 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 0)\n",
    "        classification_loss_3 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_4 = outputs[1]\n",
    "\n",
    "        loss = contrastive_loss_1 + contrastive_loss_2 + pos_inside_contrastive_loss + neg_inside_contrastive_loss + outside_robust_contrastive_loss + outside_non_robust_contrastive_loss + classification_loss_1 + classification_loss_2 + classification_loss_3 + classification_loss_4\n",
    "        return loss\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40f51e-e655-4e71-bf3d-613f8991319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disentangle_model = Disentangle_Model()\n",
    "state_dict = torch.load(\"./disentangle_model/checkpoint_distill_bert_pad.bin\")\n",
    "disentangle_model.load_state_dict(state_dict[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce4294-3ced-4eca-a8e9-f4e000e3cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disentangle_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27233e-2f86-4512-b45b-65f394d5e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, hidden_states, labels):\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "classification_model = MLP(1536, 2).cuda()\n",
    "classification_model_path = \"./disentangle_model/classification_checkpoint_pad.bin\"\n",
    "state_dict = torch.load(classification_model_path)\n",
    "classification_model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "m = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943850a-5ba8-4a7c-99ee-2bb343c8390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "from openTSNE.affinity import PerplexityBasedNN\n",
    "from openTSNE import initialization\n",
    "# from openTSNE.callbacks import ErrorLogger\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d660b-5e63-43c2-bd75-6752ea60f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"test.json\", \"r\") as f:\n",
    "    dev_lines = f.readlines()\n",
    "dev_examples = []\n",
    "context_list = []\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "line = dev_lines[2]\n",
    "y_list = []\n",
    "\n",
    "# print(line)\n",
    "sample=json.loads(line)\n",
    "ss = \"[CLS] \"\n",
    "for c in sample[\"context\"]:\n",
    "    ss = ss + c + \" [SEP] \"\n",
    "context = ss.strip()\n",
    "y_list.append(0)\n",
    "for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "    pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "    pos_list.append(pos)\n",
    "    y_list.append(1)\n",
    "for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "    neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "    neg_list.append(neg)\n",
    "    y_list.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa64a9-3b3d-4437-867e-3acafe357120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"context:\")\n",
    "print(context)\n",
    "print(\"\\n\")\n",
    "# g = \"[CLS] \" + \"How delightful, considering your class usually induces the enthusiasm of a sloth on a lazy Sunday.\" + \" [SEP]\"\n",
    "# neg_list.append(g)\n",
    "# y_list.append(3)\n",
    "\n",
    "print(\"pos:\")\n",
    "for i, pos in enumerate(pos_list):\n",
    "    print(i, pos)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"neg:\")\n",
    "for i, neg in enumerate(neg_list):\n",
    "    print(i, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bd150-5fad-4cf2-b47c-bfefc4f77c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"test.json\", \"r\") as f:\n",
    "    dev_lines = f.readlines()\n",
    "dev_examples = []\n",
    "context_list = []\n",
    "\n",
    "# line = dev_lines[1]\n",
    "\n",
    "d_pos_list = []\n",
    "d_neg_list = []\n",
    "# print(line)\n",
    "for line in dev_lines:\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    reference_list = []\n",
    "    for i, p in enumerate(sample[\"positive_responses\"][:3]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        reference_list.append(pos)\n",
    "    for i, p in enumerate(sample[\"positive_responses\"][3:]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        pos_list.append(pos)\n",
    "    for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "        neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "        neg_list.append(neg)\n",
    "    with torch.no_grad():\n",
    "        context_embedding = model_encoder.encode(context)\n",
    "        context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "        context_embedding = context_embedding.reshape(1,-1)\n",
    "        d_list = []\n",
    "        for pos in reference_list:\n",
    "            pos_embedding = model_encoder.encode(pos)       \n",
    "            pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))        \n",
    "            pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "            pos_robust_pad = torch.zeros([384,])\n",
    "            pos_robust_embedding = torch.cat([pos_robust_embedding, pos_robust_pad])\n",
    "            pos_robust_embedding = pos_robust_embedding.reshape(1,-1)\n",
    "            d = paired_cosine_distances(context_embedding, pos_robust_embedding)\n",
    "            d_list.append(d.item())\n",
    "        for pos in pos_list:\n",
    "            pos_embedding = model_encoder.encode(pos)       \n",
    "            pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))        \n",
    "            pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "            pos_robust_pad = torch.zeros([384,])\n",
    "            pos_robust_embedding = torch.cat([pos_robust_embedding, pos_robust_pad])\n",
    "            pos_robust_embedding = pos_robust_embedding.reshape(1,-1)\n",
    "            d = paired_cosine_distances(context_embedding, pos_robust_embedding)\n",
    "            d_pos_list.append(np.mean(d_list-d))\n",
    "        # print(d_list)\n",
    "        for neg in neg_list:\n",
    "            neg_embedding = model_encoder.encode(neg)\n",
    "            neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "            neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "            neg_robust_pad = torch.zeros([384,])\n",
    "            neg_robust_embedding = torch.cat([neg_robust_embedding, neg_robust_pad])\n",
    "            neg_robust_embedding = neg_robust_embedding.reshape(1,-1)\n",
    "\n",
    "            d_neg = paired_cosine_distances(context_embedding, neg_robust_embedding)\n",
    "            d_neg_list.append(np.mean(d_list-d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3a071-d85a-4b2f-b50d-b4657ad46dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for dis in d_pos_list:\n",
    "    if dis > 0:\n",
    "        count += 1\n",
    "print(count, len(d_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c562c-6bce-4b51-90a7-5cb25effd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for dis in d_neg_list:\n",
    "    if dis < 0:\n",
    "        count += 1\n",
    "print(count, len(d_neg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117e424-e4db-4a25-a3d5-6676738210ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_embedding = model_encoder.encode(context)\n",
    "    context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "    context_embedding = context_embedding.reshape(1,-1)\n",
    "    d_list = []\n",
    "    for pos in pos_list[:3]:\n",
    "        pos_embedding = model_encoder.encode(pos)       \n",
    "        pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))        \n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        pos_robust_pad = torch.zeros([384,])\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, pos_robust_pad])\n",
    "        pos_robust_embedding = pos_robust_embedding.reshape(1,-1)\n",
    "        d = paired_cosine_distances(context_embedding, pos_robust_embedding)\n",
    "        d_list.append(d.item())\n",
    "    print(d_list)\n",
    "    neg = neg_list[5]\n",
    "    neg_embedding = model_encoder.encode(neg)\n",
    "    neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "    neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "    neg_robust_pad = torch.zeros([384,])\n",
    "    neg_robust_embedding = torch.cat([neg_robust_embedding, neg_robust_pad])\n",
    "    neg_robust_embedding = neg_robust_embedding.reshape(1,-1)\n",
    "\n",
    "    d_neg = paired_cosine_distances(context_embedding, neg_robust_embedding)\n",
    "    print(d_neg)\n",
    "    print(np.mean(d_list-d_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c8340-c33a-4e42-b3fa-b51de3cb4323",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (\"train.json\", \"r\") as f:\n",
    "    dev_lines = f.readlines()\n",
    "dev_examples = []\n",
    "\n",
    "line = dev_lines[1]\n",
    "y_list = []\n",
    "\n",
    "# print(line)\n",
    "pos_dis_list = []\n",
    "neg_dis_list = []\n",
    "\n",
    "for line in dev_lines:\n",
    "    context_list = []\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    y_list.append(0)\n",
    "    for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        pos_list.append(pos)\n",
    "    for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "        neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "        neg_list.append(neg)\n",
    "    with torch.no_grad():\n",
    "        context_embedding = model_encoder.encode(context)\n",
    "        context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "        context_embedding = context_embedding.reshape(1,-1)\n",
    "        # d_list = []\n",
    "        for pos in pos_list:\n",
    "            pos_embedding = model_encoder.encode(pos)       \n",
    "            pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))        \n",
    "            pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "            pos_robust_pad = torch.zeros([384,])\n",
    "            pos_robust_embedding = torch.cat([pos_robust_embedding, pos_robust_pad])\n",
    "            pos_robust_embedding = pos_robust_embedding.reshape(1,-1)\n",
    "            d = paired_cosine_distances(context_embedding, pos_robust_embedding)\n",
    "            pos_dis_list.append(d.item())\n",
    "        for neg in neg_list:\n",
    "            neg_embedding = model_encoder.encode(neg)\n",
    "            neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "            neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "            neg_robust_pad = torch.zeros([384,])\n",
    "            neg_robust_embedding = torch.cat([neg_robust_embedding, neg_robust_pad])\n",
    "            neg_robust_embedding = neg_robust_embedding.reshape(1,-1)\n",
    "\n",
    "            d= paired_cosine_distances(context_embedding, neg_robust_embedding)\n",
    "            neg_dis_list.append(d.item())\n",
    "    # print(d_neg)\n",
    "    # print(np.mean(np.abs(d_list-d_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1bab9-6938-4dcd-b5b1-953170f90306",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pos_dis_list), np.mean(neg_dis_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb22b3-82af-4b81-966a-699d4b89879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for dis in pos_dis_list:\n",
    "    if dis <= 1.0:\n",
    "        count += 1\n",
    "print(count, len(pos_dis_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94891516-8b15-4f2b-aae4-782f3168ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4531/5710\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5c97a-2667-4594-80e2-c2bcd2cf60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for dis in neg_dis_list:\n",
    "    if dis > 0.9:\n",
    "        count += 1\n",
    "print(count, len(pos_dis_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27129309-683e-489e-abbd-4081be72b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "5141/5710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472e1ea-d766-4e03-b69c-283a82b3c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "(4531+5141)/(5710*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2b3fc-fc0a-4179-92b4-a79fc433d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_embedding = model_encoder.encode(context)\n",
    "    context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "    for pos in pos_list:\n",
    "        pos_embedding = model_encoder.encode(pos)       \n",
    "        pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))        \n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        pos_robust_pad = torch.zeros([384,])\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, pos_robust_pad])\n",
    "            \n",
    "    for neg in neg_list:\n",
    "        neg_embedding = model_encoder.encode(neg)\n",
    "        neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d50cd-201f-4c88-a0ab-b79cbb56547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_robust_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6410e85-ba27-4911-b426-f0198d27b520",
   "metadata": {},
   "source": [
    "x:原编码 x1：经过线性层之后的编码 x2：拆分之后的编码。 y：源类型 y1:拆分之后的类型（0：neg robust， 1：pos robust， 2：pos non robust, 3: neg_non_robust, 4:context_robust,  5:context_non_robust）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c0196-336d-49b8-9c96-fb42d7d7c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_embedding = model_encoder.encode(context)\n",
    "    print(context_embedding.shape)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    x_list.append(context_embedding)\n",
    "    y_list.append(0)\n",
    "    # x1_list = []\n",
    "    x1_list = []\n",
    "    context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "    # context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "    x1_list.append(context_embedding.detach().numpy())\n",
    "    x2_list = []\n",
    "    y1_list = []\n",
    "    x2_list.append(context_embedding.detach().numpy())\n",
    "    y1_list.append(4)\n",
    "    dis_list = []\n",
    "    score_list = []\n",
    "    # x2_list = [context_robust_embedding.detach().numpy()]\n",
    "    # y1_list = [0]\n",
    "    # print(context_embedding.detach().numpy().shape)\n",
    "    for pos in pos_list:\n",
    "        pos_embedding = model_encoder.encode(pos)\n",
    "        x_list.append(pos_embedding)\n",
    "        y_list.append(1)\n",
    "        pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))\n",
    "        x1_list.append(pos_response_embedding.detach().numpy())\n",
    "        \n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        pos_pad = torch.zeros([384])\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, pos_pad])\n",
    "        pos_non_robust_embedding = torch.cat([pos_non_robust_embedding, pos_pad])\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, pos_robust_embedding], dim=1)\n",
    "        # print(hidden_state.shape)\n",
    "        labels = torch.tensor([1])\n",
    "        outputs = classification_model(hidden_state, labels)\n",
    "        logits = outputs[0]\n",
    "        # print(logits)              \n",
    "        outputs = m(logits)\n",
    "        predict = outputs\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "\n",
    "        score_list.append(round(predict[0][1], 3))\n",
    "        # sample[\"score\"] = str(round(predict[0][1], 3))\n",
    "        # sample[\"distance\"] = str(d[0])\n",
    "        x2_list.append(pos_robust_embedding.detach().numpy())\n",
    "        y1_list.append(1)\n",
    "        \n",
    "        x2_list.append(pos_non_robust_embedding.detach().numpy())\n",
    "        y1_list.append(2)\n",
    "        pos_robust_embedding = pos_robust_embedding.reshape(1,-1)\n",
    "        d = paired_cosine_distances(context_embedding, pos_robust_embedding)\n",
    "        dis_list.append(d.item())\n",
    "        # pos_non_robust_list.append(pos_non_robust_embedding.detach().numpy())\n",
    "        # y1_list.append(2)\n",
    "        # x1_list.append(pos_response_embedding.detach().numpy())\n",
    "            \n",
    "    for neg in neg_list:\n",
    "\n",
    "        neg_embedding = model_encoder.encode(neg)\n",
    "        neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "        neg_robust_pad = torch.zeros([384,])\n",
    "        neg_robust_embedding = torch.cat([neg_robust_embedding, neg_robust_pad])\n",
    "        hidden_state = torch.cat([context_embedding, neg_robust_embedding], dim=1)\n",
    "        # print(hidden_state.shape)\n",
    "        labels = torch.tensor([0])\n",
    "        outputs = classification_model(hidden_state, labels)\n",
    "        logits = outputs[0]\n",
    "        # print(logits)              \n",
    "        outputs = m(logits)\n",
    "        predict = outputs\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "\n",
    "        score_list.append(round(predict[0][1], 3))\n",
    "        # sample[\"score\"] = str(round(predict[0][1], 3))\n",
    "        # sample[\"distance\"] = str(d[0])\n",
    "        x2_list.append(neg_robust_embedding.detach().numpy())\n",
    "        y1_list.append(1)\n",
    "\n",
    "        x2_list.append(neg_non_robust_embedding.detach().numpy())\n",
    "        y1_list.append(2)\n",
    "        neg_robust_embedding = neg_robust_embedding.reshape(1,-1)\n",
    "        d = paired_cosine_distances(context_embedding, neg_robust_embedding)\n",
    "        dis_list.append(d.item())\n",
    "\n",
    "\n",
    "        neg_robust_embedding = neg_robust_embedding.reshape(1,-1)\n",
    "\n",
    "        d= paired_cosine_distances(context_embedding, neg_robust_embedding)\n",
    "        neg_dis_list.append(d.item())\n",
    "x = np.array(x_list)  #经过线性层之前的编码\n",
    "y = np.array(y_list) #经过线性层前的类别\n",
    "x1 = np.array(x1_list) #经过线性层后的编码\n",
    "x2 = np.array(x2_list) #拆分后的编码\n",
    "y1 = np.array(y1_list) #拆分后的类别\n",
    "print(len(x), len(y), len(x1), len(x2), len(y1))\n",
    "# with torch.no_grad():\n",
    "#     context_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[0]))\n",
    "#     pos_response_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[1]))\n",
    "#     neg_response_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[2]))\n",
    "#     context_embedding = disentangle_model.linear(context_embedding)\n",
    "#     pos_response_embedding = disentangle_model.linear(pos_response_embedding)\n",
    "#     neg_response_embedding = disentangle_model.linear(neg_response_embedding)\n",
    "#     pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "#     neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ed0b2-a930-43ad-b5b7-fa636dee3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"train.json\", \"r\") as f:\n",
    "    dev_lines = f.readlines()\n",
    "dev_examples = []\n",
    "\n",
    "# line = dev_lines[1]\n",
    "y_list = []\n",
    "\n",
    "# print(line)\n",
    "pos_dis_list = []\n",
    "neg_dis_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "x1_list = []\n",
    "x2_list = []\n",
    "y1_list = []\n",
    "for line in dev_lines[:60]:\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        pos_list.append(pos)\n",
    "    for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "        neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "        neg_list.append(neg)\n",
    "    with torch.no_grad():\n",
    "        context_embedding = model_encoder.encode(context)\n",
    "        # print(context_embedding.shape)\n",
    "\n",
    "        x_list.append(context_embedding)\n",
    "        y_list.append(2)\n",
    "        # x1_list = []\n",
    "\n",
    "        context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "        # context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "        x1_list.append(context_embedding.detach().numpy())\n",
    "\n",
    "        # x2_list.append(context_embedding.detach().numpy())\n",
    "        y1_list.append(2)\n",
    "\n",
    "        for pos in pos_list:\n",
    "            pos_embedding = model_encoder.encode(pos)\n",
    "            x_list.append(pos_embedding)\n",
    "            y_list.append(1)\n",
    "            pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))\n",
    "            # x_list.append(pos_response_embedding.detach().numpy())\n",
    "            # y_list.append(1)\n",
    "            pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "            pos_pad = torch.zeros([384])\n",
    "            pos_robust_embedding = torch.cat([pos_robust_embedding, pos_pad])\n",
    "            pos_non_robust_embedding = torch.cat([pos_non_robust_embedding, pos_pad])\n",
    "            x1_list.append(pos_robust_embedding.detach().numpy())\n",
    "            y1_list.append(1)\n",
    "\n",
    "        for neg in neg_list:\n",
    "\n",
    "            neg_embedding = model_encoder.encode(neg)\n",
    "            x_list.append(neg_embedding)\n",
    "            y_list.append(0)\n",
    "            neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))       \n",
    "            neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "            neg_robust_pad = torch.zeros([384,])\n",
    "            neg_robust_embedding = torch.cat([neg_robust_embedding, neg_robust_pad])\n",
    "            # hidden_state = torch.cat([context_embedding, neg_robust_embedding], dim=1)\n",
    "            # sample[\"score\"] = str(round(predict[0][1], 3))\n",
    "            # sample[\"distance\"] = str(d[0])\n",
    "            x1_list.append(neg_robust_embedding.detach().numpy())\n",
    "            y1_list.append(0)\n",
    "\n",
    "x = np.array(x_list)  #经过线性层之前的编码\n",
    "y = np.array(y_list) #经过线性层前的类别\n",
    "x1 = np.array(x1_list) #经过线性层后的编码\n",
    "x2 = np.array(x2_list) #拆分后的编码\n",
    "y1 = np.array(y1_list) #拆分后的类别\n",
    "print(len(x), len(y), len(x1), len(x2), len(y1))\n",
    "# with torch.no_grad():\n",
    "#     context_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[0]))\n",
    "#     pos_response_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[1]))\n",
    "#     neg_response_embedding = torch.tensor(disentangle_model.model_encoder.encode(data[2]))\n",
    "#     context_embedding = disentangle_model.linear(context_embedding)\n",
    "#     pos_response_embedding = disentangle_model.linear(pos_response_embedding)\n",
    "#     neg_response_embedding = disentangle_model.linear(neg_response_embedding)\n",
    "#     pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "#     neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831156e-e9e4-4a9d-b4f8-44cc4da6927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"test.json\", \"r\") as f:\n",
    "    dev_lines = f.readlines()\n",
    "dev_examples = []\n",
    "context_list = []\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "line = dev_lines[2]\n",
    "y_list = []\n",
    "\n",
    "# print(line)\n",
    "sample=json.loads(line)\n",
    "ss = \"[CLS] \"\n",
    "for c in sample[\"context\"]:\n",
    "    ss = ss + c + \" [SEP] \"\n",
    "context = ss.strip()\n",
    "y_list.append(0)\n",
    "for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "    pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "    pos_list.append(pos)\n",
    "    y_list.append(1)\n",
    "for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "    neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "    neg_list.append(neg)\n",
    "    y_list.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a074b-fc97-41d8-8895-a9c06d929c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_embedding = model_encoder.encode(context)\n",
    "    print(context_embedding.shape)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    x_list.append(context_embedding)\n",
    "    y_list.append(0)\n",
    "    # x1_list = []\n",
    "    x1_list = []\n",
    "    context_embedding = disentangle_model.linear(torch.tensor(context_embedding))\n",
    "    # context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "    x1_list.append(context_embedding.detach().numpy())\n",
    "    x2_list = []\n",
    "    y1_list = []\n",
    "    x_robust_list = []\n",
    "    y_robust_list = []\n",
    "    x2_list.append(context_embedding.detach().numpy())\n",
    "    x_robust_list.append(context_embedding.detach().numpy())\n",
    "    y1_list.append(0)\n",
    "    y_robust_list.append(0)\n",
    "    # x2_list = [context_robust_embedding.detach().numpy()]\n",
    "    # y1_list = [0]\n",
    "    # print(context_embedding.detach().numpy().shape)\n",
    "    for pos in pos_list:\n",
    "        pos_embedding = model_encoder.encode(pos)\n",
    "        x_list.append(pos_embedding)\n",
    "        y_list.append(1)\n",
    "        pos_response_embedding = disentangle_model.linear(torch.tensor(pos_embedding))\n",
    "        x1_list.append(pos_response_embedding.detach().numpy())\n",
    "        \n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        pos_pad = torch.zeros([384])\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, pos_pad])\n",
    "        pos_non_robust_embedding = torch.cat([pos_non_robust_embedding, pos_pad])\n",
    "\n",
    "        \n",
    "        x2_list.append(pos_robust_embedding.detach().numpy())\n",
    "        x_robust_list.append(pos_robust_embedding.detach().numpy())\n",
    "        y1_list.append(1)\n",
    "        y_robust_list.append(1)\n",
    "        \n",
    "        x2_list.append(pos_non_robust_embedding.detach().numpy())\n",
    "        y1_list.append(2)\n",
    "        \n",
    "        # pos_non_robust_list.append(pos_non_robust_embedding.detach().numpy())\n",
    "        # y1_list.append(2)\n",
    "        # x1_list.append(pos_response_embedding.detach().numpy())\n",
    "            \n",
    "    for neg in neg_list:\n",
    "        neg_embedding = model_encoder.encode(neg)\n",
    "        x_list.append(neg_embedding)\n",
    "        neg_response_embedding = disentangle_model.linear(torch.tensor(neg_embedding))\n",
    "        x1_list.append(neg_response_embedding.detach().numpy())\n",
    "        y_list.append(2)\n",
    "        \n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "\n",
    "        neg_robust_embedding = torch.cat([neg_robust_embedding, pos_pad])\n",
    "        neg_non_robust_embedding = torch.cat([neg_non_robust_embedding, pos_pad])\n",
    "        x2_list.append(neg_non_robust_embedding.detach().numpy())\n",
    "        y1_list.append(3)\n",
    "        x2_list.append(neg_robust_embedding.detach().numpy())\n",
    "        x_robust_list.append(neg_robust_embedding.detach().numpy())\n",
    "        y_robust_list.append(2)\n",
    "        y1_list.append(0)\n",
    "        # print(x2_list)\n",
    "\n",
    "x = np.array(x_list)  #经过线性层之前的编码\n",
    "y = np.array(y_list) #经过线性层前的类别\n",
    "x1 = np.array(x1_list) #经过线性层后的编码\n",
    "x2 = np.array(x2_list) #拆分后的编码\n",
    "y1 = np.array(y1_list) #拆分后的类别\n",
    "x_robust = np.array(x_robust_list)\n",
    "y_robust = np.array(y_robust_list)\n",
    "print(len(x), len(y), len(x1), len(x2), len(y1), len(x_robust), len(y_robust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe95e61-94d9-4c7d-bfc3-04cf58aa9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=3,\n",
    "    n_iter=200,\n",
    "    metric=\"cosine\",\n",
    "    n_jobs=8,\n",
    "    random_state=0,\n",
    ")\n",
    "embedding = tsne.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33326b3d-114b-4d92-a1e8-26141bdf8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "classes = np.unique(y)\n",
    "\n",
    "default_colors = matplotlib.rcParams[\"axes.prop_cycle\"]\n",
    "colors = {k: v[\"color\"] for k, v in zip(classes, default_colors())}\n",
    "\n",
    "point_colors = list(map(colors.get, y))\n",
    "\n",
    "ax.scatter(x[:, 0], x[:, 1], c=point_colors, marker=\"v\", s=50)\n",
    "\n",
    "legend_handles = [\n",
    "    matplotlib.lines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        marker=\"s\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=colors[yi],\n",
    "        ms=10,\n",
    "        alpha=1,\n",
    "        linewidth=0,\n",
    "        markeredgecolor=\"k\",\n",
    "#         label=yi\n",
    "    )\n",
    "    for yi in classes\n",
    "]\n",
    "legend_kwargs_ = dict(labels=[\"context\", \"pos\", \"neg\"],bbox_to_anchor=(1.0, 1.0), frameon=False, fontsize=10)\n",
    "ax.legend(handles=legend_handles, **legend_kwargs_)\n",
    "ax.set_title(\"Normal\")\n",
    "plt.savefig(\"fig1_1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162c4eb-8456-40d4-992f-45a322389c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=3,\n",
    "    n_iter=200,\n",
    "    metric=\"cosine\",\n",
    "    n_jobs=8,\n",
    "    random_state=0,\n",
    ")\n",
    "embedding = tsne.fit(x_robust)\n",
    "x = embedding\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "classes = np.unique(y_robust)\n",
    "\n",
    "default_colors = matplotlib.rcParams[\"axes.prop_cycle\"]\n",
    "colors = {k: v[\"color\"] for k, v in zip(classes, default_colors())}\n",
    "\n",
    "point_colors = list(map(colors.get, y_robust))\n",
    "\n",
    "ax.scatter(x[:, 0], x[:, 1], c=point_colors, marker=\"v\", s=50)\n",
    "\n",
    "legend_handles = [\n",
    "    matplotlib.lines.Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        marker=\"s\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=colors[yi],\n",
    "        ms=10,\n",
    "        alpha=1,\n",
    "        linewidth=0,\n",
    "        markeredgecolor=\"k\",\n",
    "#         label=yi\n",
    "    )\n",
    "    for yi in classes\n",
    "]\n",
    "legend_kwargs_ = dict(labels=[\"context\", \"pos_robust\", \"neg_robust\", ],bbox_to_anchor=(0.98, 0.2), frameon=False, fontsize=10)\n",
    "ax.legend(handles=legend_handles, **legend_kwargs_)\n",
    "ax.set_title(\"Disentangled\")\n",
    "plt.savefig(\"fig1_2.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f7aa1-47dc-4481-894d-07204e1d43a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b6f9a1-be50-4123-828f-fd6bc7648ab0",
   "metadata": {},
   "source": [
    "上图为经过线性层前的编码的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15aa942-dd34-4e63-8208-bf7a938aeb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c71845-62ff-40d0-a609-7c7915934d9d",
   "metadata": {},
   "source": [
    "通过对编码前的response和编码后的response进行可视化，发现仍能有很好的区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a659ab-17a3-4301-988e-246d0d732639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4b763ba-98e9-4539-981f-7a7615d168cf",
   "metadata": {},
   "source": [
    "x:原编码 x1：经过线性层之后的编码 x2：拆分之后的编码。 y：源类型 y1:拆分之后的类型（0：neg robust， 1：pos robust， 2：pos non robust, 3: neg_non_robust）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb867c6-7aaf-4c62-bc8c-eac8ef6e6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=3,ax.set_title(\"example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6cfaa-5427-4204-a0d7-1ec2738d265f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d750eb-bde6-4d28-b74d-3efb6c3b8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, hidden_states, labels):\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b61440-2d61-40fa-88e8-a19a60ea74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"train.json\", \"r\") as f:\n",
    "    train_lines = f.readlines()\n",
    "train_classification_examples = []\n",
    "context_list = []\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "for line in train_lines:\n",
    "    # print(line)\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        train_classification_examples.append((context, pos, 1))\n",
    "    for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "        neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "        train_classification_examples.append((context, neg, 0))\n",
    "# model = SentenceTransformer(\"output/train_bi-encoder-mnrl-distilbert-base-uncased-2023-08-23_21-32-34/72340\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd54ce-bd25-4dac-8438-8eb990b9a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"test.json\", \"r\") as f:\n",
    "    test_lines = f.readlines()\n",
    "test_classification_examples = []\n",
    "context_list = []\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "for line in test_lines:\n",
    "    # print(line)\n",
    "    sample=json.loads(line)\n",
    "    ss = \"[CLS] \"\n",
    "    for c in sample[\"context\"]:\n",
    "        ss = ss + c + \" [SEP] \"\n",
    "    context = ss.strip()\n",
    "    for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "        pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "        test_classification_examples.append((context, pos, 1))\n",
    "    for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "        neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "        test_classification_examples.append((context, neg, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7b719-f4b8-488b-8b74-a28f726a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_dataloader = DataLoader(train_classification_examples, shuffle=True, batch_size=1024)\n",
    "test_classification_dataloader = DataLoader(test_classification_examples, shuffle=True, batch_size=1024)\n",
    "num_train_data = len(train_classification_examples)\n",
    "num_test_data = len(test_classification_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82969f5f-dceb-4277-8d52-cd249d937bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = MLP(1152, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce54323-09f3-4d2a-9831-3e739ad17755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_epoch = 30\n",
    "epoch = 0\n",
    "load_checkpoint_path = \"./disentangle_classifier/\"\n",
    "if not os.path.exists(load_checkpoint_path):\n",
    "   os.mkdir(load_checkpoint_path)\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=1e-5)\n",
    "for epoch in range(num_epoch):\n",
    "    accu = 0\n",
    "    for train_data in train_classification_dataloader:\n",
    "        with torch.no_grad():\n",
    "            labels = train_data[2]\n",
    "            context_embedding = torch.tensor(model_encoder.encode(train_data[0]))\n",
    "            context_embedding = disentangle_model.linear(context_embedding)\n",
    "            context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "            response_embedding = torch.tensor(model_encoder.encode(train_data[1]))\n",
    "            response_embedding = disentangle_model.linear(response_embedding)\n",
    "            non_robust_embedding, robust_embedding = response_embedding.chunk(2, -1)\n",
    "            # print(context_embedding.shape)\n",
    "            hidden_state = torch.cat([context_embedding, robust_embedding], dim=1)\n",
    "            # print(hidden_state.shape)\n",
    "\n",
    "        outputs = classification_model(hidden_state, labels)\n",
    "        # print(data[2])\n",
    "        loss = outputs[1]\n",
    "        logits = outputs[0]\n",
    "        # print(outputs)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    total_num_right = 0\n",
    "    for test_data in test_classification_dataloader:\n",
    "        with torch.no_grad():\n",
    "            labels = test_data[2]\n",
    "            context_embedding = torch.tensor(model_encoder.encode(test_data[0]))\n",
    "            context_embedding = disentangle_model.linear(context_embedding)\n",
    "            context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "            response_embedding = torch.tensor(model_encoder.encode(test_data[1]))\n",
    "            response_embedding = disentangle_model.linear(response_embedding)\n",
    "            non_robust_embedding, robust_embedding = response_embedding.chunk(2, -1)\n",
    "            # print(context_embedding.shape)\n",
    "            # non_robust_embedding, robust_embedding = response_embedding.chunk(2, -1)\n",
    "            # print(context_embedding.shape)\n",
    "            hidden_state = torch.cat([context_embedding, robust_embedding], dim=1)\n",
    "            # print(hidden_state.shape)\n",
    "            outputs = classification_model(hidden_state, labels)\n",
    "            logits = outputs[0]\n",
    "            score = logits.argmax(dim=-1)\n",
    "            num_right = ((score == labels).float()).sum()\n",
    "            total_num_right += num_right\n",
    "    right_rate = total_num_right/num_test_data\n",
    "    print(right_rate)\n",
    "    if right_rate > accu:\n",
    "        accu = right_rate\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": classification_model.state_dict(),\n",
    "            \"best_accu\": accu,\n",
    "        }\n",
    "        torch.save(state, load_checkpoint_path + \"checkpoint_v3.bin\")\n",
    "    # print(right_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a9751-df86-4511-b865-cf4d83a50ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.先对文本进行constrastive learning，并对sentence transformer模型的输出加一层线性层\n",
    "# 2.对线性层的输出再进行一次contrastive learning，保证线性层的输出与sentence transformer模型的输出差别不大，此时冻结sentence transformer模型训练，\n",
    "#   包括以后的步骤也会冻结sentence transformer模型\n",
    "# 3，对线性层的输出进行切分：robust与non robust，并做contrastive learning\n",
    "#    3.1 response内部contrastive learning：robust与non robust远离\n",
    "#    3.2 不同response之间contrastive learning：robust与robust相近\n",
    "#  对于这一步，设立一个分类器\n",
    "import json\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from enum import Enum\n",
    "# import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "import datetime\n",
    "\n",
    "def log(message, logfile=\"app.log\"):\n",
    "    \"\"\"\n",
    "    Log a message with a timestamp to a specified logfile.\n",
    "    \n",
    "    :param message: Message to be logged\n",
    "    :param logfile: File to which the log will be written (default: app.log)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(logfile, \"a\") as log_file:\n",
    "        log_file.write(f\"{timestamp} - {message}\\n\")\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        # print(hidden_states.shape)\n",
    "        n = hidden_states.shape[0]\n",
    "        a = torch.ones(n)\n",
    "        labels = labels * a\n",
    "        labels = labels.long().cuda()\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, hidden_states, labels):\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "class SiameseDistanceMetric(Enum):\n",
    "    \"\"\"\n",
    "    The metric for the contrastive loss\n",
    "    \"\"\"\n",
    "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
    "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n",
    "    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n",
    "\n",
    "class Second_MultipleNegativesRankingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        This loss expects as input a batch consisting of sentence pairs (a_1, p_1), (a_2, p_2)..., (a_n, p_n)\n",
    "        where we assume that (a_i, p_i) are a positive pair and (a_i, p_j) for i!=j a negative pair.\n",
    "\n",
    "        For each a_i, it uses all other p_j as negative samples, i.e., for a_i, we have 1 positive example (p_i) and\n",
    "        n-1 negative examples (p_j). It then minimizes the negative log-likehood for softmax normalized scores.\n",
    "\n",
    "        This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc))\n",
    "        as it will sample in each batch n-1 negative docs randomly.\n",
    "\n",
    "        The performance usually increases with increasing batch sizes.\n",
    "\n",
    "        For more information, see: https://arxiv.org/pdf/1705.00652.pdf\n",
    "        (Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4)\n",
    "\n",
    "        You can also provide one or multiple hard negatives per anchor-positive pair by structering the data like this:\n",
    "        (a_1, p_1, n_1), (a_2, p_2, n_2)\n",
    "\n",
    "        Here, n_1 is a hard negative for (a_1, p_1). The loss will use for the pair (a_i, p_i) all p_j (j!=i) and all n_j as negatives.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            model = SentenceTransformer('distilbert-base-uncased')\n",
    "            train_examples = [InputExample(texts=['Anchor 1', 'Positive 1']),\n",
    "                InputExample(texts=['Anchor 2', 'Positive 2'])]\n",
    "            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "            train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale: float = 20.0, similarity_fct = util.cos_sim):\n",
    "        \"\"\"\n",
    "        :param model: SentenceTransformer model\n",
    "        :param scale: Output of similarity function is multiplied by scale value\n",
    "        :param similarity_fct: similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot product (and then set scale to 1)\n",
    "        \"\"\"\n",
    "        super(Second_MultipleNegativesRankingLoss, self).__init__()\n",
    "        # self.model = model\n",
    "        self.scale = scale\n",
    "        self.similarity_fct = similarity_fct\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # self.distance_metric = SiameseDistanceMetric.COSINE_DISTANCE\n",
    "        # self.classify_model = nn.Linear(2 * model.get_sentence_embedding_dimension(), 3)\n",
    "\n",
    "\n",
    "    def forward(self, reps):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        embeddings_a = reps[0]\n",
    "        embeddings_b = torch.cat(reps[1:])\n",
    "        # print(embeddings_a.shape, embeddings_b.shape)\n",
    "        scores = self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n",
    "        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)  # Example a[i] should match with b[i]\n",
    "        return self.cross_entropy_loss(scores, labels)\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {'scale': self.scale, 'similarity_fct': self.similarity_fct.__name__}\n",
    "\n",
    "class test_ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the\n",
    "    two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.\n",
    "\n",
    "    Further information: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "\n",
    "    :param model: SentenceTransformer model\n",
    "    :param distance_metric: Function that returns a distance between two embeddings. The class SiameseDistanceMetric contains pre-defined metrices that can be used\n",
    "    :param margin: Negative samples (label == 0) should have a distance of at least the margin value.\n",
    "    :param size_average: Average by the size of the mini-batch.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        train_examples = [\n",
    "            InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
    "            InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
    "\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "        train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "        model.fit([(train_dataloader, train_loss)], show_progress_bar=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, distance_metric=SiameseDistanceMetric.COSINE_DISTANCE, margin: float = 0.5, size_average:bool = True):\n",
    "        super(test_ContrastiveLoss, self).__init__()\n",
    "        self.distance_metric = distance_metric\n",
    "        self.margin = margin\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        distance_metric_name = self.distance_metric.__name__\n",
    "        for name, value in vars(SiameseDistanceMetric).items():\n",
    "            if value == self.distance_metric:\n",
    "                distance_metric_name = \"SiameseDistanceMetric.{}\".format(name)\n",
    "                break\n",
    "\n",
    "        return {'distance_metric': distance_metric_name, 'margin': self.margin, 'size_average': self.size_average}\n",
    "\n",
    "    def forward(self, reps, labels):\n",
    "        # reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        # assert len(reps) == 2\n",
    "        labels = torch.tensor(labels)\n",
    "        rep_anchor, rep_other = reps\n",
    "        # print(rep_anchor.shape,rep_other.shape)\n",
    "\n",
    "        distances = self.distance_metric(rep_anchor, rep_other)\n",
    "        # print(distances)\n",
    "        losses = 0.5 * (labels.float() * distances.pow(2) + (1 - labels).float() * F.relu(self.margin - distances).pow(2))\n",
    "        # print(losses)\n",
    "        return losses.mean() if self.size_average else losses.sum()\n",
    "\n",
    "model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-distilbert-base-uncased-2023-08-23_21-32-34/72340\").cuda()\n",
    "class Disentangle_Model(nn.Module):\n",
    "    def __init__(self, max_seq_length=1024, batch_size=32, num_labels=3):\n",
    "        super(Disentangle_Model, self).__init__()\n",
    "        # self.model_name_or_path = model_name_or_path\n",
    "        # self.device = device\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_labels = num_labels\n",
    "        # self.model_encoder = SentenceTransformer(\"output/train_bi-encoder-mnrl-bert-base-uncased-2023-08-29_15-22-44/86808\")\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        self.second_mutiple_negatives_ranking_loss = Second_MultipleNegativesRankingLoss()\n",
    "        self.contrastive_loss = test_ContrastiveLoss()\n",
    "        self.classifier = Classifier(int(1536), self.num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        # self.optimizer = AdamW(self.parameters(), lr=2e-5, eps=1e-8)\n",
    "        # self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=10000)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # self.train()\n",
    "        # print(len(batch[0]))\n",
    "        with torch.no_grad():\n",
    "            context_embedding = torch.tensor(model_encoder.encode(batch[0])).cuda()\n",
    "            pos_response_embedding = torch.tensor(model_encoder.encode(batch[1])).cuda()\n",
    "            neg_response_embedding = torch.tensor(model_encoder.encode(batch[2])).cuda()\n",
    "        # print(neg_response_embedding.device)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        context_embedding = self.linear(context_embedding)\n",
    "        pos_response_embedding = self.linear(pos_response_embedding)\n",
    "        neg_response_embedding = self.linear(neg_response_embedding)\n",
    "        # print(context_embedding.shape, pos_response_embedding.shape, neg_response_embedding.shape)\n",
    "        reps = [context_embedding, pos_response_embedding, neg_response_embedding]\n",
    "        contrastive_loss_1 = self.second_mutiple_negatives_ranking_loss(reps)\n",
    "        # 3.对线性层的输出进行切分：robust与non robust，并做contrastive learning\n",
    "        pos_non_robust_embedding, pos_robust_embedding = pos_response_embedding.chunk(2, -1)\n",
    "        robust_pad = torch.zeros([pos_robust_embedding.size(0),384]).cuda() #等分切分，robust_pad==non_robust_pad\n",
    "        # robust_pad = torch.zeros([pos_robust_embedding.size(0), 256]).cuda() #不等分切分1:2(256:512)，robust_pad 256,non_robust 512\n",
    "        pos_robust_embedding = torch.cat([pos_robust_embedding, robust_pad], dim=1)\n",
    "        \n",
    "        # non_robust_pad = torch.zeros([pos_robust_embedding.size(0), 512]).cuda()\n",
    "        pos_non_robust_embedding = torch.cat([pos_non_robust_embedding, robust_pad], dim=1)\n",
    "\n",
    "        neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.chunk(2, -1)\n",
    "        # neg_non_robust_embedding, neg_robust_embedding = neg_response_embedding.split([256,512], -1)\n",
    "        neg_robust_embedding = torch.cat([neg_robust_embedding, robust_pad], dim=1)\n",
    "        neg_non_robust_embedding = torch.cat([neg_non_robust_embedding, robust_pad], dim=1)\n",
    "        pos = [pos_non_robust_embedding, pos_robust_embedding]\n",
    "        # print(pos_non_robust_embedding.shape, pos_robust_embedding.shape)\n",
    "        pos_inside_contrastive_loss = self.contrastive_loss(pos, 0.0)\n",
    "        neg = [neg_non_robust_embedding, neg_robust_embedding]\n",
    "        neg_inside_contrastive_loss = self.contrastive_loss(neg, 0.0)\n",
    "        outside_robust_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_robust_embedding], 0.0)\n",
    "        # outside_diff_contrastive_loss = self.contrastive_loss([pos_robust_embedding, neg_non_robust_embedding], 0.0)\n",
    "        outside_non_robust_contrastive_loss = self.contrastive_loss([pos_non_robust_embedding, neg_non_robust_embedding], 1.0)\n",
    "        \n",
    "        reps = [context_embedding, pos_robust_embedding, neg_robust_embedding]\n",
    "        contrastive_loss_2 = self.second_mutiple_negatives_ranking_loss(reps)\n",
    "        # 4.对于这一步，设立一个分类器\n",
    "        hidden_state = torch.cat([context_embedding, pos_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 1)\n",
    "        classification_loss_1 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, pos_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_2 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 0)\n",
    "        classification_loss_3 = outputs[1]\n",
    "\n",
    "        hidden_state = torch.cat([context_embedding, neg_non_robust_embedding], dim=1)\n",
    "        outputs = self.classifier(hidden_state, 2)\n",
    "        classification_loss_4 = outputs[1]\n",
    "\n",
    "        loss = contrastive_loss_1 + contrastive_loss_2 + pos_inside_contrastive_loss + neg_inside_contrastive_loss + outside_robust_contrastive_loss + outside_non_robust_contrastive_loss + classification_loss_1 + classification_loss_2 + classification_loss_3 + classification_loss_4\n",
    "        return loss\n",
    "\n",
    "        # self.scheduler.step()\n",
    "        # self.optimizer.zero_grad()\n",
    "        # if step % 100 == 0:\n",
    "        #     print(\"loss:\", loss.item())\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    m = nn.Softmax(dim=-1)\n",
    "    disentangle_model = Disentangle_Model().cuda()\n",
    "\n",
    "    load_checkpoint_path = \"./disentangle_model/\"\n",
    "\n",
    "    model_path = load_checkpoint_path + \"checkpoint_distill_bert_pad.bin\"\n",
    "    state_dict = torch.load(model_path)\n",
    "    disentangle_model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    # print(train_classification_examples)\n",
    "\n",
    "    with open (\"train.json\", \"r\") as f:\n",
    "        test_lines = f.readlines()\n",
    "    test_classification_examples = []\n",
    "    context_list = []\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    for line in test_lines:\n",
    "        # print(line)\n",
    "        sample=json.loads(line)\n",
    "        ss = \"[CLS] \"\n",
    "        for c in sample[\"context\"]:\n",
    "            ss = ss + c + \" [SEP] \"\n",
    "        context = ss.strip()\n",
    "        for i, p in enumerate(sample[\"positive_responses\"]):\n",
    "            pos = \"[CLS] \" + p + \" [SEP]\"\n",
    "            test_classification_examples.append((context, pos, 1))\n",
    "        for j, n in enumerate(sample[\"adversarial_negative_responses\"]):\n",
    "            neg = \"[CLS] \" + n + \" [SEP]\"\n",
    "            test_classification_examples.append((context, neg, 0))\n",
    "\n",
    "\n",
    "    test_classification_dataloader = DataLoader(test_classification_examples, shuffle=False, batch_size=1)\n",
    "\n",
    "    num_test_data = len(test_classification_examples)\n",
    "    \n",
    "    classification_model = MLP(1536, 2).cuda()\n",
    "    classification_model_path = load_checkpoint_path + \"classification_checkpoint_pad.bin\"\n",
    "    state_dict = torch.load(classification_model_path)\n",
    "    classification_model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    total_num_right = 0\n",
    "    total_num_right_1 = 0\n",
    "    samples = []\n",
    "    for test_data in test_classification_dataloader:\n",
    "        sample = {}\n",
    "        with torch.no_grad():\n",
    "            labels = test_data[2].to(\"cuda\")\n",
    "            # print(test_data)\n",
    "            sample[\"context\"] = test_data[0]\n",
    "            sample[\"response\"] = test_data[1]\n",
    "            sample[\"label\"] = test_data[2].item()\n",
    "            context_embedding = torch.tensor(model_encoder.encode(test_data[0])).cuda()\n",
    "            context_embedding = disentangle_model.linear(context_embedding)\n",
    "            # context_non_robust_embedding, context_robust_embedding = context_embedding.chunk(2,-1)\n",
    "            response_embedding = torch.tensor(model_encoder.encode(test_data[1])).cuda()\n",
    "            response_embedding = disentangle_model.linear(response_embedding)\n",
    "            non_robust_embedding, robust_embedding = response_embedding.chunk(2, -1)\n",
    "            # pos_pad = torch.zeros([robust_embedding.size(0), 384]).cuda()\n",
    "            # non_robust_embedding, robust_embedding = response_embedding.split([256,512], -1)\n",
    "            # print(robust_embedding.shape)\n",
    "            robust_pad = torch.zeros([robust_embedding.size(0), 384]).cuda()\n",
    "            # robust_pad = torch.zeros([robust_embedding.size(0),256]).cuda()\n",
    "            # print(robust_pad.shape)\n",
    "            # print(robust_embedding.shape)\n",
    "            robust_embedding = torch.cat([robust_embedding, robust_pad], dim=1)\n",
    "\n",
    "            # pos_dis_list.append(d.item())\n",
    "            # print(context_embedding.shape)\n",
    "            # non_robust_embedding, robust_embedding = response_embedding.chunk(2, -1)\n",
    "            # print(context_embedding.shape)\n",
    "            hidden_state = torch.cat([context_embedding, robust_embedding], dim=1)\n",
    "            # print(hidden_state.shape)\n",
    "            outputs = classification_model(hidden_state, labels)\n",
    "            logits = outputs[0]\n",
    "            # print(logits)              \n",
    "            outputs = m(logits)\n",
    "            predict = outputs\n",
    "            predict = predict.detach().cpu().numpy()\n",
    "            # print(predict)\n",
    "            # pred = np.argmax(predict, axis=1).flatten()\n",
    "            # print(pred[0])\n",
    "            score = logits.argmax(dim=-1)\n",
    "            # num_right = ((score == labels).float()).sum()\n",
    "            a = robust_embedding.cpu().reshape(1,-1)\n",
    "            b = context_embedding.cpu().reshape(1,-1)\n",
    "            d = paired_cosine_distances(a, b)\n",
    "            # total_num_right += num_right\n",
    "            sample[\"predicted\"] = score.item()\n",
    "            \n",
    "            sample[\"predicted_score\"] = str(round(predict[0][1], 3))\n",
    "            sample[\"distance\"] = str(d[0])\n",
    "            # if d[0] < 0.9 and int(sample[\"predicted\"]) == 0:\n",
    "            #     # total_num_right += 1\n",
    "            #     sample[\"predicted\"] = 1\n",
    "            # if d[0] > 0.9 and int(sample[\"predicted\"]) == 1:\n",
    "            #     # total_num_right += 1\n",
    "            #     sample[\"predicted\"] = 0\n",
    "            samples.append(sample)\n",
    "    # right_rate = total_num_right/num_test_data\n",
    "    # print(right_rate)\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for s in samples:\n",
    "    if int(s[\"label\"]) == 1:\n",
    "        total_pos += 1\n",
    "        if int(s[\"predicted\"]) == 1:\n",
    "            pos_right += 1\n",
    "    if int(s[\"label\"]) == 0:\n",
    "        total_neg += 1\n",
    "        if int(s[\"predicted\"]) == 0:\n",
    "            neg_right += 1\n",
    "    if int(s[\"label\"]) == int(s[\"predicted\"]):\n",
    "        total_num_right += 1\n",
    "right_rate = total_num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg)\n",
    "        \n",
    "json_samples = json.dumps(samples, ensure_ascii=False, indent=2)\n",
    "with open(\"train_classification_result_small_distance.json\", 'w', encoding='utf-8') as f:\n",
    "    f.write(json_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e53427-7d9a-4362-bd4a-29bcd9119fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.layer1 = nn.Linear(hidden_size, 2)\n",
    "        self.layer3 = nn.Linear(2, 2, bias=False)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        # print(hidden_states.shape)\n",
    "        n = hidden_states.shape[0]\n",
    "        a = torch.ones(n).cuda()\n",
    "        labels = labels * a\n",
    "        labels = labels.long()\n",
    "        logits = self.layer3(hidden_states)\n",
    "        \n",
    "        # logits = self.relu(value1)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "Gate = Classifier(2, 2).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeb630-87c6-4408-945d-c97ca5cd6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_classification_result_small_distance.json\", \"r\") as f:\n",
    "    lines = json.load(f)\n",
    "train_examples = []\n",
    "for line in lines:\n",
    "    train_examples.append((torch.tensor([float(line[\"predicted_score\"]), float(line[\"distance\"])]), line[\"label\"]))\n",
    "    \n",
    "with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "    lines = json.load(f)\n",
    "test_examples = []\n",
    "for line in lines:\n",
    "    test_examples.append((torch.tensor([float(line[\"score\"]), float(line[\"distance\"])]), line[\"label\"]))\n",
    "    \n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=32)\n",
    "num_train_data = len(train_examples)\n",
    "num_test_data = len(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391032f-c04f-46da-8275-b472362f8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor((1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c1f89-9b41-4dc3-8367-e3fda1234b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52459989-fa16-4c9b-a40a-f56fff47f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(Gate.parameters(), lr=1e-6)\n",
    "num_epoch = 10000\n",
    "# import os\n",
    "for epoch in range(num_epoch):\n",
    "    accu = 0\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    print(\"\\n\")\n",
    "    print(\"*\" * 20, \"epoch: \", epoch, \" Training\", \"*\" * 20)\n",
    "    for train_data in train_dataloader:\n",
    "\n",
    "        data = train_data[0].cuda()\n",
    "        # print(data)\n",
    "        labels = train_data[1].to(\"cuda\")\n",
    "        # print(labels.shape)\n",
    "        # h = torch.cat([train_data[0].unsqueeze(0),train_data[1].unsqueeze(0)],dim=0)\n",
    "        # print(h.shape)\n",
    "        # print(h)\n",
    "        outputs = Gate(data, labels)\n",
    "        # print(data[2])\n",
    "        loss = outputs[1]\n",
    "        logits = outputs[0]\n",
    "        # print(outputs)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.update(1)\n",
    "    total_num_right = 0\n",
    "    print(\"\\n\")\n",
    "    pbar = tqdm(test_dataloader)\n",
    "    print(\"*\" * 20, \"loss: \", loss.item(), \"*\" * 20)\n",
    "    print(\"*\" * 20, \"epoch: \", epoch, \" Testing\", \"*\" * 20)\n",
    "    for test_data in test_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = test_data[0].cuda()\n",
    "            labels = test_data[1].to(\"cuda\")\n",
    "\n",
    "            outputs = Gate(data, labels)\n",
    "            logits = outputs[0]\n",
    "            score = logits.argmax(dim=-1)\n",
    "            num_right = ((score == labels).float()).sum()\n",
    "            total_num_right += num_right\n",
    "        pbar.update(1)\n",
    "    right_rate = total_num_right/num_test_data\n",
    "    print(right_rate)\n",
    "    print(Gate.state_dict())\n",
    "    # log(right_rate, logfile)\n",
    "    if right_rate > accu:\n",
    "        accu = right_rate\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": Gate.state_dict(),\n",
    "            \"best_accu\": accu,\n",
    "        }\n",
    "        torch.save(state, load_checkpoint_path + \"classification_checkpoint_pad_gate.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05874d-8c38-4683-a6a0-53d1c08cc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "\n",
    "distances = []\n",
    "for s in samples:\n",
    "    distances.append(float(s[\"distance\"]))\n",
    "print(np.mean(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad18a15-1822-42a9-856e-099fb66a4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_classification_result_small_distance.json\", \"r\") as f:\n",
    "#     samples = json.load(f)\n",
    "# num_right = 0\n",
    "# pos_right = 0\n",
    "# neg_right = 0\n",
    "# total_pos = 0\n",
    "# total_neg = 0\n",
    "accu = 0\n",
    "p_value = 0\n",
    "for p in range(0, 10, 1):\n",
    "    p = p / 10\n",
    "    with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "        samples = json.load(f)\n",
    "    num_right = 0\n",
    "    pos_right = 0\n",
    "    neg_right = 0\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    for i, s in enumerate(samples):\n",
    "        if float(s[\"distance\"]) < p and int(s[\"label\"]) == 1:\n",
    "            num_right += 1\n",
    "        if float(s[\"distance\"]) > p and int(s[\"label\"]) == 0:\n",
    "            num_right += 1\n",
    "\n",
    "    right_rate = num_right/len(samples)\n",
    "    print(p,right_rate)\n",
    "    if accu < right_rate:\n",
    "        accu = right_rate\n",
    "        p_value = p\n",
    "\n",
    "print(accu, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50b79f-4280-4db2-9677-785efa93fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"dev_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "for i, s in enumerate(samples):\n",
    "    if float(s[\"distance\"]) < p and int(s[\"label\"]) == 1:\n",
    "        num_right += 1\n",
    "    if float(s[\"distance\"]) > p and int(s[\"label\"]) == 0:\n",
    "        num_right += 1\n",
    "\n",
    "right_rate = num_right/len(samples)\n",
    "print(p,right_rate)\n",
    "if accu < right_rate:\n",
    "    accu = right_rate\n",
    "    p_value = p\n",
    "\n",
    "print(accu, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557d25f-91c2-4c61-a130-037090fb182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "total_num_right = 0\n",
    "for s in samples:\n",
    "    if int(s[\"label\"]) == 1:\n",
    "        total_pos += 1\n",
    "        if int(s[\"predicted\"]) == 1:\n",
    "            pos_right += 1\n",
    "    if int(s[\"label\"]) == 0:\n",
    "        total_neg += 1\n",
    "        if int(s[\"predicted\"]) == 0:\n",
    "            neg_right += 1\n",
    "    if int(s[\"label\"]) == int(s[\"predicted\"]):\n",
    "        total_num_right += 1\n",
    "right_rate = total_num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b655f6-e269-4e4e-9952-e7ef4d6d2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import datetime\n",
    "def log(message, logfile=\"app.log\"):\n",
    "    \"\"\"\n",
    "    Log a message with a timestamp to a specified logfile.\n",
    "    \n",
    "    :param message: Message to be logged\n",
    "    :param logfile: File to which the log will be written (default: app.log)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(logfile, \"a\") as log_file:\n",
    "        log_file.write(f\"{timestamp} - {message}\\n\")\n",
    "with open(\"classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "random.shuffle(samples)\n",
    "mean_distance = 0\n",
    "logfile = \"threshold_value_test.log\"\n",
    "message = \"使用均值动态求阈值\"\n",
    "with open(logfile, \"w\") as log_file:\n",
    "    log_file.write(f\"{message}\\n\")\n",
    "for i, s in enumerate(samples):\n",
    "    if float(s[\"distance\"]) < 0.9 and int(s[\"predicted\"]) == 0:\n",
    "        s[\"predicted\"] = 1\n",
    "    # if float(s[\"distance\"]) > 1.4 and int(s[\"predicted\"]) == 1:\n",
    "    #     s[\"predicted\"] = 0\n",
    "    if int(s[\"label\"]) == 1:\n",
    "        total_pos += 1\n",
    "        if int(s[\"predicted\"]) == 1:\n",
    "            pos_right += 1\n",
    "    if int(s[\"label\"]) == 0:\n",
    "        total_neg += 1\n",
    "        if int(s[\"predicted\"]) == 0:\n",
    "            neg_right += 1\n",
    "    if int(s[\"label\"]) == int(s[\"predicted\"]):\n",
    "        num_right += 1\n",
    "    # if int(s[\"label\"]) != int(s[\"predicted\"]):\n",
    "    #     print(i, s)\n",
    "right_rate = num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64361c53-88a5-47fb-a45b-3f548cc2a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "def log(message, logfile=\"app.log\"):\n",
    "    \"\"\"\n",
    "    Log a message with a timestamp to a specified logfile.\n",
    "    \n",
    "    :param message: Message to be logged\n",
    "    :param logfile: File to which the log will be written (default: app.log)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(logfile, \"a\") as log_file:\n",
    "        log_file.write(f\"{message}\\n\")\n",
    "with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "random.shuffle(samples)\n",
    "mean_distance = float(samples[0][\"distance\"])\n",
    "distances=[]\n",
    "distances.append(float(samples[0][\"distance\"]))\n",
    "logfile = \"threshold_value_test.log\"\n",
    "message = \"使用均值动态求阈值\"\n",
    "log(message, logfile)\n",
    "for i, s in enumerate(samples):\n",
    "    mean_distance = np.mean(distances)\n",
    "    mean_distance = round(mean_distance, 1)\n",
    "    num_right = 0\n",
    "    pos_right = 0\n",
    "    neg_right = 0\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    num_right_raw = 0\n",
    "    examples = copy.deepcopy(samples[:i+1])\n",
    "    distances = []\n",
    "    for j, ss in enumerate(examples):\n",
    "        if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "            num_right_raw += 1\n",
    "        if float(ss[\"distance\"]) < mean_distance and int(ss[\"predicted\"]) == 0:\n",
    "            ss[\"predicted\"] = 1\n",
    "        # if float(s[\"distance\"]) > 1.4 and int(s[\"predicted\"]) == 1:\n",
    "        #     s[\"predicted\"] = 0\n",
    "        if int(ss[\"label\"]) == 1:\n",
    "            total_pos += 1\n",
    "            if int(ss[\"predicted\"]) == 1:\n",
    "                pos_right += 1\n",
    "        if int(ss[\"label\"]) == 0:\n",
    "            total_neg += 1\n",
    "            if int(ss[\"predicted\"]) == 0:\n",
    "                neg_right += 1\n",
    "        if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "            num_right += 1\n",
    "        distances.append(float(ss[\"distance\"]))\n",
    "\n",
    "    a = num_right/len(examples) * 100\n",
    "    a = round(a, 2)\n",
    "    \n",
    "    b = num_right_raw/len(examples) * 100\n",
    "    b = round(b, 2)\n",
    "    examples  = []\n",
    "    print(a, b, mean_distance)\n",
    "    m = \"{}----\".format(i) + \"{}----\".format(a) + \"{}----\".format(b) + \"{}\".format(mean_distance)\n",
    "    log(m, logfile)\n",
    "    # if int(s[\"label\"]) != int(s[\"predicted\"]):\n",
    "    #     print(i, s)\n",
    "right_rate = num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg, num_right_raw/num_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04e1d5-96b0-406a-8f38-df13f9a5accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "def log(message, logfile=\"app.log\"):\n",
    "    \"\"\"\n",
    "    Log a message with a timestamp to a specified logfile.\n",
    "    \n",
    "    :param message: Message to be logged\n",
    "    :param logfile: File to which the log will be written (default: app.log)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(logfile, \"a\") as log_file:\n",
    "        log_file.write(f\"{message}\\n\")\n",
    "with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "num_test_data = len(samples)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "random.shuffle(samples)\n",
    "mean_distance = float(samples[0][\"distance\"])\n",
    "distances=[]\n",
    "distances.append(float(samples[0][\"distance\"]))\n",
    "logfile = \"threshold_value_test_1.log\"\n",
    "message = \"使用均值动态求阈值\"\n",
    "log(message, logfile)\n",
    "for i, s in enumerate(samples):\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    mean_distance = round(mean_distance, 2)\n",
    "    num_right = 0\n",
    "    pos_right = 0\n",
    "    neg_right = 0\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    num_right_raw = 0\n",
    "    examples = copy.deepcopy(samples[:i+1])\n",
    "    distances = []\n",
    "    for j, ss in enumerate(examples):\n",
    "        if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "            num_right_raw += 1\n",
    "        # h = (np.abs(float(ss[\"distance\"]) - mean_distance)) + float(ss[\"score\"])\n",
    "        # h = float(ss[\"score\"])/float(ss[\"distance\"])\n",
    "        h = (float(ss[\"score\"]) - float(ss[\"distance\"])) * (float(ss[\"distance\"])/std_distance) * (float(ss[\"score\"]) - float(ss[\"distance\"]))\n",
    "        # if float(ss[\"distance\"]) < mean_distance and int(ss[\"predicted\"]) == 0:\n",
    "        #     ss[\"predicted\"] = 1\n",
    "        # if float(s[\"distance\"]) > 1.4 and int(s[\"predicted\"]) == 1:\n",
    "        #     s[\"predicted\"] = 0\n",
    "        if int(ss[\"label\"]) == 1:\n",
    "            total_pos += 1\n",
    "            if h < 0.5:\n",
    "                pos_right += 1\n",
    "        if int(ss[\"label\"]) == 0:\n",
    "            total_neg += 1\n",
    "            if h > 0.5:\n",
    "                neg_right += 1\n",
    "        # if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "        #     num_right += 1\n",
    "        distances.append(float(ss[\"distance\"]))\n",
    "    num_right = neg_right + pos_right\n",
    "    a = num_right/len(examples) * 100\n",
    "    a = round(a, 2)\n",
    "    \n",
    "    b = num_right_raw/len(examples) * 100\n",
    "    b = round(b, 2)\n",
    "    examples  = []\n",
    "    # print(a, b, mean_distance)\n",
    "    m = \"{}----\".format(i) + \"{}----\".format(a) + \"{}----\".format(b) + \"{}\".format(mean_distance)\n",
    "    log(m, logfile)\n",
    "    # if int(s[\"label\"]) != int(s[\"predicted\"]):\n",
    "    #     print(i, s)\n",
    "right_rate = num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg, num_right_raw/num_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bb23d-1b9c-4067-bdfb-beca2e6fb0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "# def log(message, logfile=\"app.log\"):\n",
    "#     \"\"\"\n",
    "#     Log a message with a timestamp to a specified logfile.\n",
    "    \n",
    "#     :param message: Message to be logged\n",
    "#     :param logfile: File to which the log will be written (default: app.log)\n",
    "#     \"\"\"\n",
    "#     timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     with open(logfile, \"a\") as log_file:\n",
    "#         log_file.write(f\"{message}\\n\")\n",
    "with open(\"test_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples = json.load(f)\n",
    "examples = []\n",
    "num_test_data = len(samples)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "random.shuffle(samples)\n",
    "mean_distance = float(samples[0][\"distance\"])\n",
    "distances=[]\n",
    "# distances.append(float(samples[0][\"distance\"]))\n",
    "# logfile = \"threshold_value_test.log\"\n",
    "# message = \"使用均值动态求阈值\"\n",
    "# log(message, logfile)\n",
    "num_right = 0\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "num_right_raw = 0\n",
    "with open(\"train_classification_result_small_distance.json\", \"r\") as f:\n",
    "    samples1 = json.load(f)\n",
    "for s in samples1:\n",
    "    distances.append(float(s[\"distance\"]))\n",
    "mean_distance = np.mean(distances)\n",
    "std_distance = np.std(distances)\n",
    "max_distance = np.max(distances)\n",
    "min_distance = np.min(distances)\n",
    "d_list = []\n",
    "for i, ss in enumerate(samples):\n",
    "    if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "        num_right_raw += 1\n",
    "    # h = (np.abs(float(ss[\"distance\"]) - mean_distance)) + float(ss[\"score\"])\n",
    "    # h = float(ss[\"score\"])/float(ss[\"distance\"])\n",
    "    # h = float(ss[\"distance\"]) - float(ss[\"score\"])\n",
    "    distance_norm = (float(ss[\"distance\"]) -min_distance)/(max_distance-min_distance)\n",
    "    \n",
    "    h = (float(ss[\"distance\"]) -min_distance)/(max_distance-min_distance)\n",
    "    l = (1 - h) + float(ss[\"score\"])\n",
    "    # l = (2 * (1 - h) * float(ss[\"score\"]))/((1 - h) + float(ss[\"score\"]))\n",
    "    # h = -(float(ss[\"score\"]) - mean_distance) * (float(ss[\"distance\"])/std_distance) * (float(ss[\"score\"]) - mean_distance)\n",
    "    # h = -(float(ss[\"score\"]) - mean_distance) * (float(ss[\"distance\"])/std_distance) * (float(ss[\"score\"]) \n",
    "    #     - mean_distance) + mean_distance/std_distance - mean_distance\n",
    "    # if float(ss[\"distance\"]) < mean_distance and int(ss[\"predicted\"]) == 0:\n",
    "    #     ss[\"predicted\"] = 1\n",
    "    # if float(s[\"distance\"]) > 1.4 and int(s[\"predicted\"]) == 1:\n",
    "    #     s[\"predicted\"] = 0\n",
    "    if l > 0.5:\n",
    "        ss[\"modified_predicted\"] = 1\n",
    "    else:\n",
    "        ss[\"modified_predicted\"] = 0\n",
    "    if int(ss[\"label\"]) == 1:\n",
    "        total_pos += 1\n",
    "        if l >= 0.5:\n",
    "            pos_right += 1\n",
    "    if int(ss[\"label\"]) == 0:\n",
    "        total_neg += 1\n",
    "        if l < 0.5:\n",
    "            neg_right += 1 \n",
    "        \n",
    "    # if int(ss[\"label\"]) == int(ss[\"predicted\"]):\n",
    "    #     num_right += 1\n",
    "    d_list.append(float(ss[\"distance\"]))\n",
    "    ss[\"h\"] = h\n",
    "    ss[\"slm_d\"] = l\n",
    "    ss[\"norm_dis\"] = distance_norm\n",
    "    examples.append(ss)\n",
    "    \n",
    "\n",
    "num_right = neg_right + pos_right\n",
    "a = num_right/len(samples) * 100\n",
    "a = round(a, 2)\n",
    "\n",
    "b = num_right_raw/len(samples) * 100\n",
    "b = round(b, 2)\n",
    "# examples  = []\n",
    "    # print(a, b, mean_distance)\n",
    "    # m = \"{}----\".format(i) + \"{}----\".format(a) + \"{}----\".format(b) + \"{}\".format(mean_distance)\n",
    "    # log(m, logfile)\n",
    "    # if int(s[\"label\"]) != int(s[\"predicted\"]):\n",
    "    #     print(i, s)\n",
    "right_rate = num_right/num_test_data\n",
    "print(right_rate, pos_right/total_pos, neg_right/total_neg, num_right_raw/num_test_data,np.mean(d_list),mean_distance, std_distance)\n",
    "\n",
    "\n",
    "# json_samples = json.dumps(examples, ensure_ascii=False, indent=2)\n",
    "# with open(\"test_classification_result_small_distance_h_normalized.json\", 'w', encoding='utf-8') as f:\n",
    "#     f.write(json_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disentangle",
   "language": "python",
   "name": "disentangle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
